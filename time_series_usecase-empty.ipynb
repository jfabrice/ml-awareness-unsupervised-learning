{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Awareness - Time Series use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is following the progression of the Unsupervised Learning Awareness for AI Decision-makers. It provides practical illustrations in Python to understand the notions we have seen in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Author: Fabrice JIMENEZ\n",
    "    \n",
    "Link to course materials: https://github.com/jfabrice/ml-awareness-unsupervised-learning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary loading with Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using this notebook with Google Colab, please execute first the following cells, to retrieve the GitHub repository content and set the working directory. Otherwise, ignore these 3 cells and move to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jfabrice/ml-awareness-unsupervised-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('ml-awareness-unsupervised-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Imports and dataset presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index('time', inplace=True)\n",
    "print(\"Shape of our dataset: \"+str(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "# 2- Preprocessing, data quality control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### High level exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cycles do we have? How many points per cycle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cycle').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 15 cycles, with very different number of points per cycle.\n",
    "Let's see what our parameter looks like within a cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle']==1]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle']==3]['p1'].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem to be continuous (close in time = close in value) which is logical if they are physical parameters.\n",
    "\n",
    "The parameter seems to behave quite differently in different cycles."
   ]
  },
  {
   "source": [
    "### Missing values or uneven time steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Are there missing values?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Are the time steps all equal?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale = pd.Series(df.index)\n",
    "timescale.diff().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time steps are not always equal to 1 second..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check an example of gap of 7 seconds, to see what strategy we could use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['p1']['2001-01-10 00:10:30':'2001-01-10 00:11:30'].plot(style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that values still seem to be continuous. This kind of gap can be interpolated with mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: We can resample data with equal steps of 1 second within each cycle, with mean interpolation (oversampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = []\n",
    "\n",
    "# For each cycle, create a small dataframe with interpolation\n",
    "for c in df['cycle'].unique():\n",
    "    tmp = df[df['cycle'] == c].copy()\n",
    "    tmp = tmp.resample('1S').mean().interpolate(method='linear')\n",
    "    \n",
    "    newdf.append(tmp)\n",
    "    \n",
    "# Concatenate the results to have the complete dataframe\n",
    "newdf = pd.concat(newdf, axis=0)\n",
    "df = newdf.copy()\n",
    "del newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we only have time steps of 1 second or 1 day (between cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale = pd.Series(df.index)\n",
    "timescale.diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['p1']['2001-01-10 00:10:30':'2001-01-10 00:11:30'].plot(style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have clearly defined samples (windows) which contain time series regularly indexed."
   ]
  },
  {
   "source": [
    "### Signal regularity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the parameter values closely, we have a lot of constant steps, making the overall shape piecewise constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle'] == 1]['p1'][150:400].plot(style='o', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, looking at the global shapes of signals, this kind of behavior will only add noise or pollution when we want to study the dynamics of the signal, for instance the frequencies, or the derivative. Let's filter our signals by applying a rolling mean, with a suitable window size making our signals smooth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cycle'] == 1]['p1'][150:400].rolling('10S').mean().plot(style='r-o', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it on all cycles and all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = []\n",
    "\n",
    "# For each cycle, create a small dataframe with rolling mean\n",
    "for c in df['cycle'].unique():\n",
    "    tmp = df[df['cycle'] == c].copy()\n",
    "    tmp = tmp.rolling('10S').mean()\n",
    "    \n",
    "    newdf.append(tmp)\n",
    "    \n",
    "# Concatenate the results to have the complete dataframe\n",
    "newdf = pd.concat(newdf, axis=0)\n",
    "df = newdf.copy()\n",
    "del newdf\n",
    "\n",
    "df[\"cycle\"] = df[\"cycle\"].astype(int)"
   ]
  },
  {
   "source": [
    "We now have smooth continuous time series for 15 flights, without missing values and with constant step size!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3- Feature engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the task we need to achieve, we need to detect abnormal cycles. Therefore we need to have a dataset where each cycle is described by a set of features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "What features can now be used to summarize each cycle?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe a cycle we want to estimate:\n",
    "   - The distribution of parameter values: info contained in summary statistics of p1, p2, p3, p4 (mean, std...)\n",
    "   - The dynamics - evolution of parameter values: info contained in the derivatives of these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be applied to the dataframe of each cycle\n",
    "def computeFeatures(tab):\n",
    "    res = pd.Series()\n",
    "    \n",
    "    # For each time series we compute the following features\n",
    "    for p in [c for c in tab.columns if c != 'cycle']:\n",
    "        # Mean\n",
    "        res[p+'_mean'] = tab[p].mean()\n",
    "        res[p+'_diff_mean'] = tab[p].diff().mean()\n",
    "        # 1st decile\n",
    "        res[p+'_d1'] = tab[p].quantile(0.1)\n",
    "        res[p+'_diff_d1'] = tab[p].diff().quantile(0.1)\n",
    "        # 9th decile\n",
    "        res[p+'_d9'] = tab[p].quantile(0.9)\n",
    "        res[p+'_diff_d9'] = tab[p].diff().quantile(0.9)\n",
    "        # Standard deviation\n",
    "        res[p+'_std'] = tab[p].std()\n",
    "        res[p+'_diff_std'] = tab[p].diff().std()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the feature computation on each cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.groupby('cycle').apply(computeFeatures)\n",
    "print(\"Shape of the features dataset: \"+str(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "source": [
    "Now let's go back to class!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Pattern visualisation"
   ]
  },
  {
   "source": [
    "Let's compute a PCA to look at patterns & correlations between our features!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca import pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pca(n_components=5)\n",
    "resPCA = model.fit_transform((features - features.mean())/features.std(), verbose=0)['PC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resPCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = model.biplot(n_feat=5, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = model.biplot3d(n_feat=5, legend=False)"
   ]
  },
  {
   "source": [
    "Now let's go back to class!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5- Grouping similar cycles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform clustering on the 5 PCA components, let's use a hierarchical clustering to see how many groups we should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hac = hierarchy.linkage(resPCA, method='ward', metric='euclidean')\n",
    "dendrogram = hierarchy.dendrogram(hac, labels=features.index.map(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A choice of 5 clusters seems reasonable! Let's cut the dendrogram at 4 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['cluster'] = hierarchy.cut_tree(hac,5).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the clusters in the PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCx = 'PC1'\n",
    "PCy = 'PC2'\n",
    "\n",
    "###############################\n",
    "colors = ['blue','red','orange','cyan', 'magenta']\n",
    "fig, ax = plt.subplots()\n",
    "xl = plt.xlabel(PCx)\n",
    "yl = plt.ylabel(PCy)\n",
    "for i in range(len(resPCA)):\n",
    "    row = resPCA.iloc[i]\n",
    "    ax.scatter(row[PCx], row[PCy], c=colors[int(features['cluster'].iloc[i])], alpha=0.7)\n",
    "    ax.text(x=row[PCx]+0.1, y=row[PCy]+0.1, s=str(int(features.index[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect cycles 3, 4 and 8 to be different from the others, let's see!"
   ]
  },
  {
   "source": [
    "Now let's go back to class!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Identifying abnormal cycles"
   ]
  },
  {
   "source": [
    "Since we don't have any reference of normality, we can directly exclude the novelty detection approach. We are forced to adopt outlier detection, considering that anomalies are cycles which are different from the majority."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's compute an Isolation Forest anomaly score and visualize it on the PCA projection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(n_estimators=200).fit(resPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['score'] = clf.decision_function(resPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(x=resPCA['PC1'],y=resPCA['PC2'], c=-features['score'], cmap='Reds')\n",
    "fig.colorbar(sc, label='anomaly score')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title(\"Isolation Forest Score\")\n",
    "\n",
    "for i in range(len(resPCA)):\n",
    "    row = resPCA.iloc[i]\n",
    "    ax.text(x=row[PCx]+0.1, y=row[PCy]+0.1, s=str(int(features.index[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-features['score']).plot(style='bo-', alpha=0.7)"
   ]
  },
  {
   "source": [
    "We have our usual suspects: cycles 3 and 8 with good confidence, and cycles 4 and 15 with more uncertainty."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 7- Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our model, we need either: \n",
    "- a validation or test set, containing the ground truth (the clusters we had to find, the real anomalies...) - <b>Strong validation strategy</b>\n",
    "- a human interpretation of the results (an expert eye who is able to tell if the clusters make sense, or if the anomalies detected are real ones...) - <b>Weak validation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the frame of this use case, we don't have any validation dataset, so we would rely only on a human interpretation by an expert. We would give the expert the parameters or behaviors we suspect to be anomalies. According to their interpretation, we can go back in the different steps of this analysis to change some elements (different resampling, smoothing, choice of features, method for clustering...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why it is very important not to get lost in your choices! To be able to play with these choices, to adjust according to the results validation."
   ]
  },
  {
   "source": [
    "Now let's go back to class to conclude!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}